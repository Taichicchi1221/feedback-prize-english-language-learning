hydra:
  run:
    dir: "."

globals:
  seed: 42
  debug: False

tokenizer:
  max_length: 512
  path: ${model.path}
  params:

model:
  path: "microsoft/deberta-v3-base"
  freeze_embeddings: True
  freeze_encoders: 6
  params:
    hidden_dropout: 0.1
    hidden_dropout_prob: 0.1
    attention_dropout: 0.1
    attention_probs_dropout_prob: 0.1

collator:
  mlm: True
  mlm_probability: 0.15

training:
  output_dir: .
  overwrite_output_dir: True
  fp16: True
  bf16: False
  num_train_epochs: 5
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  lr_scheduler_type: cosine
  evaluation_strategy: epoch
  logging_strategy: epoch
  save_strategy: epoch
  eval_steps: 1
  logging_steps: 1
  save_steps: 1
  metric_for_best_model: eval_loss
  greater_is_better: False
  prediction_loss_only: True
  report_to: none

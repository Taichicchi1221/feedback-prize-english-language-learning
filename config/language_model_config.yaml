hydra:
  run:
    dir: "."

globals:
  seed: 42
  n_fold: 5
  use_folds: null
  epochs: 5
  pseudo_label_epochs: 0
  steps_per_epoch: -1 # must be set in script
  total_steps: -1 # must be set in script
  debug: False

tokenizer:
  max_length:
    train: 512
    test: 512
  path: ${model.encoder.path}
  params:

preprocessor:
  method: original_text

dataloader:
  train:
    batch_size: 16
    shuffle: True
    drop_last: True
    pin_memory: True
    num_workers: 4
  test:
    batch_size: 16
    shuffle: False
    drop_last: False
    pin_memory: False
    num_workers: 0

trainer:
  train:
    max_epochs: ${globals.epochs}
    accelerator: gpu
    devices: 1
    precision: 16
    strategy:
    gradient_clip_algorithm: norm
    gradient_clip_val: 
    accumulate_grad_batches: 2
    amp_backend: native
    amp_level:
    val_check_interval: 0.2
    num_sanity_val_steps: 0
  predict:
    precision: 32
    accelerator: ${trainer.train.accelerator}
    devices: ${trainer.train.devices}
  pseudo_label_train:
    max_epochs: ${globals.pseudo_label_epochs}
    accelerator: ${trainer.train.accelerator}
    devices: ${trainer.train.devices}
    precision: ${trainer.train.precision}
    strategy: ${trainer.train.strategy}
    gradient_clip_algorithm: ${trainer.train.gradient_clip_algorithm}
    gradient_clip_val: ${trainer.train.gradient_clip_val}
    accumulate_grad_batches: ${trainer.train.accumulate_grad_batches}
    amp_backend: ${trainer.train.amp_backend}
    amp_level: ${trainer.train.amp_level}
    val_check_interval: 1
    num_sanity_val_steps: ${trainer.train.num_sanity_val_steps}


config:
  path: ${model.encoder.path}

model:
  encoder:
    path: microsoft/deberta-v3-base
    num_reinit_layers: 1
    num_freeze_layers: 0
    params:
      hidden_dropout: 0.0
      hidden_dropout_prob: 0.0
      attention_dropout: 0.0
      attention_probs_dropout_prob: 0.0
      layer_norm_eps: 1.0e-12
  head:
    type: MeanPoolingHead
    init: True
    params:
      # dropout_rate: 0.5 # SimpleHead, MultiSampleDropoutHead, MeanPoolingMultiSampleDropoutHead
      # dropout_num: 5 # MultiSampleDropoutHead, MeanPoolingMultiSampleDropoutHead
      # num_hidden_layers: 3 # CLSConcatHead, WeightedLayerPoolingHead
      # weights: null # WeightedLayerPoolingHead
      # hidden_features: 1024 # AttentionHead, MaskAddedAttentionHead
      # dropout: 0.0 # LSTMHead, GRUHead
      

loss:
  type: MCRMSELoss # [MCRMSELoss, SmoothL1Loss, ScaledMCBCELoss]
  params:

metric:
  type: MCRMSEMetric
  params:
    num_cols: 6

optimizer:
  type: AdamW # [SGD, Adam, AdamW, RAdam, AdaBelief]
  lr:
    encoder: 24.0e-05
    head: 24.0e-05
  lr_decay_rate: 0.10
  weight_decay: 0.0
  params:
    # momentum: 0.0
    # eps: 1.0e-06
    # betas: [0.9, 0.999] # RAdam
  
scheduler:
  ### torch
  # type: CosineAnnealingWarmRestarts # [CosineAnnealingLR, CosineAnnealingWarmRestarts]
  # interval: step # [epoch, step]
  # params:
  #   # T_max: ${globals.steps_per_epoch} # CosineAnnealingLR
  #   T_0: ${globals.steps_per_epoch} # CosineAnnealingWarmRestarts
  #   T_mult: 2 # CosineAnnealingWarmRestarts
  #   eta_min: 1.0e-16 # CosineAnnealingLR, CosineAnnealingWarmRestarts
  #   verbose: False
  
  ### transformers
  type: cosine # ['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup']
  interval: step # [epoch, step]
  params:
    num_warmup_steps: ${globals.steps_per_epoch}
    num_training_steps: ${globals.total_steps}


pseudo_label_optimizer:
  type: ${optimizer.type} # [SGD, Adam, AdamW, RAdam]
  lr:
    encoder: 1.0e-05
    head: 1.0e-05
  lr_decay_rate: ${optimizer.lr_decay_rate}
  weight_decay: ${optimizer.weight_decay}
  params:
    # momentum: ${optimizer.params.momentum}
    # eps: ${optimizer.params.eps}
    # betas: ${optimizer.params.betas} # RAdam

pseudo_label_scheduler:
  type: null
  interval: null
  params: null

awp:
  apply: False
  params:
    adv_param: "weight"
    adv_lr: 1.0
    adv_eps: 0.001
    start_epoch: 0
    adv_step: 1

sift:
  apply: False


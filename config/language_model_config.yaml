hydra:
  run:
    dir: "."

globals:
  seed: 42
  n_fold: 5
  use_folds: null
  epochs: 5
  pseudo_label_epochs: 0
  steps_per_epoch: -1 # must be set in script
  total_steps: -1 # must be set in script
  debug: False

tokenizer:
  max_length:
    train: 512
    test: 512
  path: ${model.encoder.path}
  params:

preprocessor:
  method: original_text

dataloader:
  train:
    batch_size: 16
    shuffle: True
    drop_last: True
    pin_memory: True
    num_workers: 4
  test:
    batch_size: 16
    shuffle: False
    drop_last: False
    pin_memory: False
    num_workers: 0

trainer:
  train:
    max_epochs: ${globals.epochs}
    accelerator: gpu
    devices: 1
    precision: 16
    strategy:
    amp_backend: native
    amp_level:
    val_check_interval: 0.2
    num_sanity_val_steps: 0
  predict:
    precision: 32
    accelerator: ${trainer.train.accelerator}
    devices: ${trainer.train.devices}
  pseudo_label_train:
    max_epochs: ${globals.pseudo_label_epochs}
    accelerator: ${trainer.train.accelerator}
    devices: ${trainer.train.devices}
    precision: ${trainer.train.precision}
    strategy: ${trainer.train.strategy}
    amp_backend: ${trainer.train.amp_backend}
    amp_level: ${trainer.train.amp_level}
    val_check_interval: 1
    num_sanity_val_steps: ${trainer.train.num_sanity_val_steps}


config:
  path: ${model.encoder.path}

model:
  encoder:
    path: microsoft/deberta-v3-base
    num_reinit_layers: 0
    num_freeze_layers: 0
    params:
      hidden_dropout: 0.0
      hidden_dropout_prob: 0.0
      attention_dropout: 0.0
      attention_probs_dropout_prob: 0.0
      layer_norm_eps: 1.0e-12
  head:
    pooling:
      type: MeanPooling # {CLSPooling, CLSConcatPooling, MeanPooling, MaxPooling, MeanMaxConcatPooling, AttentionPooling, WeightedLayerPooling, LSTMPooling, GRUPooling}
      init: True
      params:
        # num_hidden_layers: 4 # WeightedLayerPooling, CLSConcatPooling
        # weights: null # WeightedLayerPooling
        # dropout: 0.0 # LSTMPooling, GRUPooling
    regressor:
      type: SimpleRegressor
      init: True
      params:
        dropout_rate: 0.0
        # dropout_num: 10

loss:
  type: MCRMSELoss # [MCRMSELoss, SmoothL1Loss, ScaledMCBCELoss]
  params:

metric:
  type: MCRMSEMetric
  params:
    num_cols: 6

optimizer:
  type: AdamW # [SGD, Adam, AdamW, RAdam, MADGRAD]
  pretrained_weight_decay: False
  lr:
    encoder: 16.0e-05
    head: 16.0e-05
  lr_decay_rate: 0.10
  weight_decay: 0.0
  accumulate_grad_batches: 1
  gradient_clip_algorithm: null
  gradient_clip_val: null
  params:
    correct_bias: False # AdamW
    # momentum: 0.0
    # eps: 1.0e-06
    # betas: [0.9, 0.999]
  
scheduler:
  ### torch
  # type: CosineAnnealingWarmRestarts # [CosineAnnealingLR, CosineAnnealingWarmRestarts]
  # interval: step # [epoch, step]
  # params:
  #   # T_max: ${globals.steps_per_epoch} # CosineAnnealingLR
  #   T_0: ${globals.steps_per_epoch} # CosineAnnealingWarmRestarts
  #   T_mult: 2 # CosineAnnealingWarmRestarts
  #   eta_min: 1.0e-16 # CosineAnnealingLR, CosineAnnealingWarmRestarts
  #   verbose: False
  
  ### transformers
  type: cosine # ['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup']
  interval: step # [epoch, step]
  params:
    num_warmup_steps: ${globals.steps_per_epoch}
    num_training_steps: ${globals.total_steps}


pseudo_label_optimizer:
  type: ${optimizer.type} # [SGD, Adam, AdamW, RAdam]
  lr:
    encoder: 1.0e-05
    head: 1.0e-05
  lr_decay_rate: ${optimizer.lr_decay_rate}
  weight_decay: ${optimizer.weight_decay}
  accumulate_grad_batches: ${optimizer.accumulate_grad_batches}
  gradient_clip_algorithm: ${optimizer.gradient_clip_algorithm}
  gradient_clip_val: ${optimizer.gradient_clip_val}
  params:
    # momentum: ${optimizer.params.momentum}
    # eps: ${optimizer.params.eps}
    # betas: ${optimizer.params.betas} # RAdam

pseudo_label_scheduler:
  type: null
  interval: null
  params: null

swa:
  # swa_lrs: 0.01
  # swa_epoch_start: 3

awp:
  # adv_param: "weight"
  # adv_lr: 1.0
  # adv_eps: 0.001
  # start_epoch: 3
  # adv_step: 1

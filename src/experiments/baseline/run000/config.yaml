hydra:
  run:
    dir: "."

globals:
  seed: 42
  n_fold: 5
  debug: False

tokenizer:
  path: ${model.encoder.path}
  max_length: 512
  params: {}

dataloader:
  steps_per_epoch: -1 # must be set in script
  train:
    batch_size: 16
    shuffle: True
    drop_last: True
    pin_memory: True
    num_workers: 1
  valid:
    batch_size: 32
    shuffle: False
    drop_last: False
    pin_memory: False
    num_workers: 0
  test:
    batch_size: 32
    shuffle: False
    drop_last: False
    pin_memory: False
    num_workers: 0

trainer:
  accelerator: gpu
  devices: 1
  precision: bf16
  max_epochs: 5
  strategy:
  gradient_clip_algorithm: norm
  gradient_clip_val:
  accumulate_grad_batches: 1
  amp_backend: native
  amp_level:
  num_sanity_val_steps: 0
  val_check_interval: 0.2

predict_trainer:
  accelerator: ${trainer.accelerator}
  devices: ${trainer.devices}
  # precision: ${trainer.precision}
  # amp_backend: ${trainer.amp_backend}
  # amp_level: ${trainer.amp_level}

model:
  encoder:
    path: "microsoft/deberta-v3-base"
    params:
      hidden_dropout: 0.0
      hidden_dropout_prob: 0.0
      attention_dropout: 0.0
      attention_probs_dropout_prob: 0.0
  head:
    type: MeanPoolingHead
    params:
      {}
      # dropout_rate: 0.00 # SimpleHead, MultiSampleDropoutHead
      # dropout_num: 5 # MultiSampleDropoutHead
      # hidden_features: 1024 # AttentionHead, MaskAddedAttentionHead
      # hidden_size: 256 # CNNHead
      # kernel_size: 8 # CNNHead
      # dropout: 0.0 # LSTMHead, GRUHead
loss:
  type: MCRMSELoss
  params: {}

metric:
  type: MCRMSEMetric
  mode: min
  params:
    num_cols: 6

optimizer:
  type: AdamW # [SGD, Adam, AdamW, RAdam, Lamb, SAM]
  # base_optimizer_name: Adam
  lr:
    encoder: 1.0e-06
    head: 1.0e-06
  weight_decay: 0.0
  params:
    {}
    # momentum: 0.0
    # betas: [0.9, 0.999] # Lamb, RAdam
    # eps: 1.0e-07 # Lamb, RAdam
    # adam: True # Lamb
    # debias: False # Lamb
    # rho: 0.05 # SAM
    # adaptive: False # SAM

scheduler:
  type: OneCycleLR # [ReduceLROnPlateau, CosineAnnealingLR, CosineAnnealingWarmRestarts, OneCycleLR]
  interval: step # [epoch, step]
  params:
    # mode: min # ReduceLROnPlateau
    # factor: 0.1 # ReduceLROnPlateau
    # patience: 2 # ReduceLROnPlateau
    # eps: 1.0e-08 # ReduceLROnPlateau
    # T_max: ${dataloader.steps_per_epoch} # CosineAnnealingLR
    # T_0: ${dataloader.steps_per_epoch} # CosineAnnealingWarmRestarts
    # T_mult: 2 # CosineAnnealingWarmRestarts
    # eta_min: 1.0e-12 # CosineAnnealingLR, CosineAnnealingWarmRestarts
    max_lr: 2.0e-05 # OneCycleLR
    pct_start: 0.1 # OneCycleLR
    steps_per_epoch: ${dataloader.steps_per_epoch} # OneCycleLR
    epochs: ${trainer.max_epochs} # OneCycleLR
    anneal_strategy: cos # OneCycleLR
    div_factor: 1.0e+02 # OneCycleLR
    final_div_factor: 1 # OneCycleLR
    verbose: False
